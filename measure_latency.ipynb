{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import yaml\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from enot.latency import SearchSpaceLatencyContainer\n",
    "from enot.latency.search_space_latency_calculator import SearchSpacePytorchCpuLatencyCalculator as CPUEstimator\n",
    "from enot.latency.search_space_latency_calculator import SearchSpacePytorchCudaLatencyCalculator as CUDAEstimator\n",
    "from enot.logging import prepare_log\n",
    "from enot.models import SearchSpaceModel\n",
    "from enot.models import SearchVariantsContainer\n",
    "from thop import profile\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from enot.latency import min_latency, max_latency, mean_latency, sample_latencies\n",
    "from enot.latency.latency_calculator import MacCalculatorFvcore\n",
    "from enot.latency import initialize_latency\n",
    "from enot.visualization import plot_latency_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__, torch.version.cuda, np.__version__)\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.yolo import Model\n",
    "from models import yolo\n",
    "\n",
    "yolo.LOGGER = prepare_log(log_format='%(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_torch_cuda():\n",
    "\n",
    "    torch_backends = torch.backends\n",
    "\n",
    "    if hasattr(torch_backends, 'cuda'):\n",
    "        cuda = torch_backends.cuda\n",
    "        cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    if hasattr(torch_backends, 'cudnn'):\n",
    "        cudnn = torch_backends.cudnn\n",
    "        cudnn.benchmark = True\n",
    "        cudnn.deterministic = False\n",
    "        cudnn.allow_tf32 = True\n",
    "        cudnn.enabled = True\n",
    "    else:\n",
    "        logging.warning('cudnn is not available')\n",
    "\n",
    "    torch.use_deterministic_algorithms(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 640\n",
    "batch_size = 1\n",
    "device = 'cpu'\n",
    "dtype = torch.float16\n",
    "\n",
    "dtype = torch.float32 if device == 'cpu' else dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cuda' in device:\n",
    "    init_torch_cuda(detect_anomaly=False)\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "torch.cuda.set_device('cuda:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select model to profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_config = 'models/hub/yolov5l6.yaml'\n",
    "# model_config = 'models/yolov5lss_v2.yaml'\n",
    "model_config = 'models/yolov5sss_v1.yaml'\n",
    "# model_config = 'models/yolov5s.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_directory = Path('.').absolute()\n",
    "model_config_file = root_directory / model_config\n",
    "\n",
    "model = Model(model_config_file).to(device)\n",
    "model.eval();\n",
    "model.fuse();\n",
    "\n",
    "search_space = None\n",
    "if any(isinstance(layer, SearchVariantsContainer) for layer in model.modules()):\n",
    "    search_space = SearchSpaceModel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optionally save test search space checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = model if search_space is None else search_space\n",
    "model_save = deepcopy(model_to_save).cpu().float()\n",
    "\n",
    "for i in range(12):\n",
    "    model_save = model_to_save.get_network_by_indexes([i] * 8)\n",
    "    ckpt = {\n",
    "        'model': model_save,\n",
    "        'ema': model_save,\n",
    "        'phase_name': 'tune',\n",
    "    }\n",
    "\n",
    "    torch.save(ckpt, f'weights/yolov5sss_{i}.pt')\n",
    "    del model_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = torch.zeros(batch_size, 3, image_size, image_size, dtype=dtype, device=device)\n",
    "# model_input = model_input.to(memory_format=torch.channels_last)\n",
    "\n",
    "model.to(dtype=dtype, device=device)\n",
    "# model = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 704\n",
    "\n",
    "while image_size >= 224:\n",
    "\n",
    "    batch_size = 1\n",
    "\n",
    "    model_input = torch.zeros(batch_size, 3, image_size, image_size, dtype=dtype, device=device)\n",
    "    # model_input = model_input.to(memory_format=torch.channels_last)\n",
    "\n",
    "    model.to(dtype=dtype, device=device)\n",
    "    # model = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "    estimator_class = CPUEstimator if device == 'cpu' else CUDAEstimator\n",
    "    estimator = estimator_class(search_space, warmup_iterations=1, run_iterations=1)\n",
    "    with torch.no_grad():\n",
    "        latency_container = estimator.compute((model_input, ))\n",
    "\n",
    "    latency_container.save_to_file(f'latency/yolov5lss_v2_cpu_bs1/r{image_size}.pkl')\n",
    "\n",
    "    image_size -= 32\n",
    "\n",
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from enot.latency import min_latency, max_latency, mean_latency, sample_latencies\n",
    "\n",
    "\n",
    "for lc_file in sorted(glob.glob('latency/yolov5lss_v2_cpu_bs1/r*.pkl')):\n",
    "\n",
    "    latency_container = SearchSpaceLatencyContainer.load_from_file(lc_file)\n",
    "    min_l, max_l, mean_l = \\\n",
    "        min_latency(latency_container), \\\n",
    "        max_latency(latency_container), \\\n",
    "        mean_latency(latency_container)\n",
    "\n",
    "    print(lc_file)\n",
    "    print(f'{min_l:.2f} {mean_l:.2f} {max_l:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency_container = initialize_latency('mmac.fvcore', search_space, (model_input, ))\n",
    "latency_container.save_to_file('latency/yolov5sss_v3_latency_flops.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space.normal_forward(True)\n",
    "\n",
    "estimator = CPUEstimator(search_space, warmup_iterations=10, run_iterations=80)\n",
    "with torch.no_grad():\n",
    "    latency_container = estimator.compute((model_input, ))\n",
    "\n",
    "latency_container.save_to_file('latency/yolov5sss_v3_latency_cpu.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency_container = SearchSpaceLatencyContainer.load_from_file('latency/yolov5sss_v3_latency_cpu.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency_container = SearchSpaceLatencyContainer.load_from_file('latency/yolov5sss_v3_latency_flops.pkl')\n",
    "\n",
    "min_l, max_l, mean_l, baseline_latency = \\\n",
    "    min_latency(latency_container), \\\n",
    "    max_latency(latency_container), \\\n",
    "    mean_latency(latency_container), \\\n",
    "    latency_container.constant_latency + sum(x[0] for x in latency_container.operations_latencies)\n",
    "\n",
    "print(f'baseline={baseline_latency:.2f}, min={min_l:.2f}, mean={mean_l:.2f} max={max_l:.2f}')\n",
    "\n",
    "latencies = np.array(sample_latencies(latency_container, n=200000, ))\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "plt.xlabel('Latency, ms')\n",
    "plt.ylabel('# cases')\n",
    "\n",
    "plt.hist(latencies, bins=100, color='k', edgecolor='k', alpha=0.2)\n",
    "\n",
    "plt.axvline(min_l, color='r', linestyle='dashed', linewidth=2, label='Min latency')\n",
    "plt.axvline(max_l, color='r', linestyle='dashed', linewidth=2, label='Max latency')\n",
    "\n",
    "plt.axvline(latencies.mean(), color='g', linestyle='dashed', linewidth=2, label='Average latency')\n",
    "# plt.axvline(np.median(latencies), color='c', linestyle='dashed', linewidth=2, label='Median latency')\n",
    "\n",
    "plt.axvline(baseline_latency, color='k', linestyle='dashed', linewidth=2, label='Baseline latency')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plot_latency_heatmap(latency_container, annotate_values=True, figsize=(16, 5));\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real, pred = [], []\n",
    "\n",
    "for _ in tqdm(range(100)):\n",
    "\n",
    "    sr = np.random.randint(8, size=8)\n",
    "\n",
    "    lat1 = latency_container.constant_latency + sum(x[i] for x, i in zip(latency_container.operations_latencies, sr))\n",
    "\n",
    "    m = search_space.get_network_by_indexes(sr)\n",
    "    lat2 = get_operation_latency(\n",
    "        m,\n",
    "        (model_input,),\n",
    "        operation_kwargs={},\n",
    "        n_iterations=5,\n",
    "        warmup_iterations=3,\n",
    "        min_iterations=5,\n",
    "        max_compute_time=20.0,\n",
    "    )\n",
    "\n",
    "    real.append(lat2); pred.append(lat1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(real) * 1000 / batch_size\n",
    "y = np.array(pred) - (1.93 - 1.39)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "\n",
    "z = np.linspace(min(np.min(x), np.min(y)), max(np.max(x), np.max(y)), 2)\n",
    "plt.plot(z, z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latency_heatmap(latency_container, annotate_values=True, figsize=(16, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for _ in range(3):\n",
    "        model(model_input)\n",
    "\n",
    "    for _ in range(3):\n",
    "        with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "            with record_function(\"model_inference\"):\n",
    "                model(model_input)\n",
    "\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        with record_function(\"model_inference\"):\n",
    "            for _ in range(1):\n",
    "                model(model_input)\n",
    "                torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model profiling for selecting entities to replace with search blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=100, max_src_column_width=150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = Path('.').absolute()\n",
    "model_config_file = root_directory / model_config\n",
    "\n",
    "model = Model(model_config_file)\n",
    "model.eval();\n",
    "model.fuse();\n",
    "model.to(dtype=dtype, device=device);\n",
    "\n",
    "search_space = None\n",
    "if any(isinstance(layer, SearchVariantsContainer) for layer in model.modules()):\n",
    "    search_space = SearchSpaceModel(model)\n",
    "\n",
    "from models import yolo\n",
    "from enot.logging import prepare_log\n",
    "yolo.LOGGER = prepare_log(log_format='%(message)s')\n",
    "\n",
    "with torch.no_grad():\n",
    "    model(model_input, profile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model('models/yolov5s.yaml').float()\n",
    "model.eval();\n",
    "model.fuse();\n",
    "\n",
    "baseline_latency = MacCalculatorFvcore().calculate(model, (model_input.float().cpu(), ))\n",
    "print(baseline_latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.export_chrome_trace(\"trace3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yolov5)",
   "language": "python",
   "name": "yolov5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
